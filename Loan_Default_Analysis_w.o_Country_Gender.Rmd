---
title: "Data Analysis w/o Imbalanced Data"
author: "Wisdom"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load Data}
loan_df <- readRDS("kiva_loan_data")
```

```{r Variable Selection}
names(loan_df)
loan_df <- loan_df %>%
    select(LOAN_AMOUNT, STATUS, SECTOR, DISBURSAL_CURRENCY, CURRENCY_EXCHANGE, REPAYMENT_INTERVAL, NUM_OWNERS)
```

```{r train test split}
set.seed(5)

indexSet <- sample(2, nrow(loan_df), replace = TRUE, prob = c(0.7, 0.3))
train_df <- loan_df[indexSet == 1, ]
test_df <- loan_df[indexSet == 2, ]

dim(train_df)
dim(test_df)
```



```{r elastic regularization}
library(glmnet)
x_train <- model.matrix(STATUS ~ ., train_df )
y_train <- if_else(train_df$STATUS == "defaulted", 1, 0)
list_of_fits <- list()
for (i in 0:10){
    fit.name <- paste0("alpha", i/10)
    list_of_fits[[fit.name]] <- cv.glmnet(x = x_train, y = y_train, alpha = i/10,
                                       family = "binomial", type.measure = "class")
}

results <- data.frame()

x_test <- model.matrix(STATUS ~ ., test_df )

for(i in 0:10){
    fit.name <- paste0("alpha", i/10)
    fit.probs <- predict(list_of_fits[[fit.name]], 
                         s = list_of_fits[[fit.name]]$lambda.1se, newx = x_test, 
                         type = "response")
    fit.preds <- rep("paid", nrow(test_df))
    fit.preds[fit.probs > 0.5] <- "defaulted"
    test_error <- mean(fit.preds != test_df$STATUS)
    temp <- data.frame(alpha = i/10, error_rate = test_error, fit_name = fit.name)
    results <- rbind(results, temp)
}

results

stargazer(results,
          title = ("best alpha value - Model 2"),
          type = "text",
          summary = FALSE)
```

```{r best fit logistic model}
# fitting the model with an alpha level of 0

model.fit <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0, 
                       type.measure = "class")

model.probs <- predict(model.fit, s = model.fit$lambda.1se, newx = x_test, 
                       type = "response")

model.preds <- rep("paid", nrow(test_df))

model.preds[model.probs >0.5] <- "defaulted"

x_tab <- table(Predicted = model.preds, True = test_df$STATUS)

lr2_cm <-confusionMatrix(x_tab)

# Getting the important Variables
lr2_coef_df <- get_coef(model.fit)

coef2_df_n5 <- lr2_coef_df %>%
    arrange(desc(coefficient)) %>%
    slice(1:5)


stargazer(coef2_df_n5,
          title = ("Top 5 Variables"),
          type = "text",
          summary = FALSE)

#Plotting the ROC Curve

lr2_roc <- roc(test_df$STATUS, model.probs, plot = T, legacy.axes=TRUE, percent=TRUE, 
                  xlab="False Positive Percentage", ylab="True Postive Percentage", 
                  lwd=4)
par(pty = "s")
plot(lr2_roc, main = "ROC Curve Logistic Reg. Model 2", col = "#4daf4a", print.auc=TRUE)

#Getting the performance metrics
stargazer(modelmetric(lr2_cm, lr2_roc),
          title = ("Model 2 Performance - Logistic Regression"),
          type = "text",
          summary = FALSE)
```


```{r}
predicted.data <- data.frame(
    probability.of.default = model.probs[,1],
    Status = test_df$STATUS
)

predicted.data <- predicted.data[
    order(predicted.data$probability.of.default, decreasing = FALSE),]

predicted.data$rank <- 1:nrow(predicted.data)

ggplot(data = predicted.data, aes(x = rank, y = probability.of.default))+
    geom_point(aes(color = Status), alpha = 1, shape = 4, stroke = 2)+
     theme_bw()+
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())+
    xlab("Index")+
    ylab("Predicted probability of default")
```



```{r Decision Trees}
library(rpart)
library(rpart.plot)

tree.fit <- rpart(STATUS ~ ., data = train_df, method = "class")

rpart.plot(tree.fit, extra = 106)

tree.pred <- predict(tree.fit, test_df, type = "class")

x_tab <- table(Predicted = tree.pred, True = test_df$STATUS)

confusionMatrix(x_tab)

train_df %>%
    ggplot(aes(fct_infreq(DISBURSAL_CURRENCY), fill = STATUS))+
    geom_bar(position = "dodge")+
    theme_bw()+
    #coord_flip()+
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())+
    labs(x = "Currency",
         y = NULL,
         title = "Number of Observations per Disbursed Currency")


```

```{r Pruning the tree}

stargazer(printcp(tree.fit),
          title = ("Model 2 - Cost Complexity Table"),
          type = "text",
          summary = FALSE)

plotcp(tree.fit)

cp <- tree.fit$cptable[which.min(tree.fit$cptable[,"xerror"]), "CP"]

ptree <- prune(tree.fit, cp)

rpart.plot(tree.fit)

ptree.pred <- predict(ptree, test_df, type = "class")

dt2_fitted <- predict(ptree, test, type = "prob")

dt2_x_tab <- table(Predicted = ptree.pred, True = test_df$STATUS)

dt2_cm <- confusionMatrix(dt2_x_tab)

#Plot ROC

dt2_roc <- roc(test_df$STATUS, dt2_fitted[,1], plot = T, legacy.axes=TRUE, percent=TRUE, 
                  xlab="False Positive Percentage", ylab="True Postive Percentage", 
                  lwd=4)
par(pty = "s")
plot(dt2_roc, main = "Model 2 - ROC Curve Decision Tree", col = "#4daf4a", print.auc=TRUE)

stargazer(modelmetric(dt2_cm, dt2_roc),
          title = ("Model 2 Performance - Decision Tree"),
          type = "text",
          summary = FALSE)

#getting the variable importance

var_impt2 <- get_var_impt(ptree)

var_impt2 %>%
    ggplot(aes(x = reorder(Variable,  - `Gini Index`), y = `Gini Index`))+
    geom_col()+
    coord_flip()+
    theme_bw()+
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())+
    labs(
        x = "variable",
        title = "Variable Importance"
    )

```


```{r Random Forest}
library(randomForest)

rf.fit <- randomForest(STATUS ~ ., data = train_df)
rf.fit

rf_pred <- predict(rf.fit, newdata = test_df, type = "class")

rf2_fitted <- predict(rf.fit, newdata = test_df, type = "prob")

rf2_x_tab <- table(Predicted = rf_pred, True = test$STATUS)

rf2_cm <- confusionMatrix(rf2_x_tab)

#Plotting the ROC Curve

rf2_roc <- roc(test_df$STATUS, rf2_fitted[,1], plot = T, legacy.axes=TRUE, percent=TRUE, 
                  xlab="False Positive Percentage", ylab="True Postive Percentage", 
                  lwd=4)
par(pty = "s")
plot(rf2_roc, main = "Model 2 - Random Forest ROC Curve", col = "#4daf4a", print.auc=TRUE)

#Getting the performance metrics
stargazer(rf_modelmetric(rf2_cm, rf2_roc, rf.fit),
          title = ("Model 2 Performance - Random Forest"),
          type = "text",
          summary = FALSE,
          digits = 3)

#Identifying variable importance
rf_importance2 <- data.frame(rf.fit$importance)
rf_importance2 <- rownames_to_column(rf_importance2, "Variable")

rf_importance2 %>%
    ggplot(aes(x = reorder(Variable, -MeanDecreaseGini), y = MeanDecreaseGini))+
    geom_col()+
    theme_bw()+
    coord_flip()+
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())+
    labs(
        title = "Variable Importance",
        x = "Variable"
    )
```

```{r Support Vector Classifier}
library(e1071)

svm.class.fit <- tune(svm, STATUS ~ ., data = train_df, kernel = "linear",
                  ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))

summary(svm.class.fit)

saveRDS(svm.class.fit, "svc2")

svc2 <- readRDS("svc2")

stargazer(svc2$performances,
          title = ("Model 2 - SVC Parameter Tuning"),
          type = "text",
          summary = FALSE,
          digits = 4)

summary(svc2)

svc2_bestmod <- svc2$best.model

summary(svc2_bestmod)

svc2_pred <- predict(svc2_bestmod, test_df)

svc2_x_tab <- table(Predicted = svc2_pred, True = test_df$STATUS)

svc2_cm <- confusionMatrix(svc2_x_tab)

svc2_cm

#ROC Plot
svc2.fitted <-  attributes(
    predict(svc2_bestmod, test_df, decision.values = TRUE)
    )$decision.values

svc2_roc <- roc(test_df$STATUS, svc2.fitted, plot = T, legacy.axes=TRUE, percent=TRUE, 
                  xlab="False Positive Percentage", ylab="True Postive Percentage", 
                  col="#4daf4a", lwd=4, print.auc=TRUE)


par(pty = "s")
plot(svc2_roc, main = "Model 2 - SVC ROC Curve", col = "#4daf4a", print.auc=TRUE)

# Getting Model Performance Metrics

modelmetric(svc2_cm, svc2_roc)

stargazer(modelmetric(svc2_cm, svc2_roc),
          title = ("Model 2 - SVC Performance"),
          type = "text",
          summary = FALSE)

```


```{r SVM}
svm.fit <- tune(svm, STATUS ~ ., data = train_df, kernel = "radial",
                ranges = list(
                    cost = c( 1, 5, 10),
                    gamma = c(0.5, 1, 5)
                )
            )


saveRDS(svm.fit, "svm2")

svm2 <- readRDS("svm2")

stargazer(svm2$performances,
          title = ("Model 2 - SVM Parameter Tuning"),
          type = "text",
          summary = FALSE,
          digits = 4)

summary(svm2)

svm2_bestmod <- svm2$best.model

svm2_pred <- predict(svm2_bestmod, test_df)

svm2_x_tab <- table(Predicted = svm2_pred, True = test_df$STATUS)

svm2_cm <- confusionMatrix(svm2_x_tab)

svm2_cm

#ROC Plot
svm2.fitted <-  attributes(
    predict(svm2_bestmod, test_df, decision.values = TRUE)
    )$decision.values

svm2_roc <- roc(test_df$STATUS, svm2.fitted, plot = T, legacy.axes=TRUE, percent=TRUE, 
                  xlab="False Positive Percentage", ylab="True Postive Percentage", 
                  col="#4daf4a", lwd=4, print.auc=TRUE)


par(pty = "s")
plot(svm2_roc, main = "Model 2 - SVM ROC Curve", col = "#4daf4a", print.auc=TRUE)

# Getting Model Performance Metrics

modelmetric(svm2_cm, svm2_roc)

stargazer(modelmetric(svm2_cm, svm2_roc),
          title = ("Model 2 - SVM Performance"),
          type = "text",
          summary = FALSE)

```

